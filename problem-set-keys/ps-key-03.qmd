---
title: "R Bootcamp Problem Set 3"
author: "Insert your name here"
---

## Setup

Start by loading libraries you need analysis in the code chunk below.
When in doubt, start by loading the tidyverse package.

```{r}
#| label: setup
library(tidyverse)
library(here)
```

## Problem Set

Each problem below is worth **5 points**.

Use the data files in the `data/` directory to answer the questions.

For this problem set, you are allowed to help each other, but you are not allowed to post correct answers in slack.

**The problem set is due 12pm on Aug 31.**

### Grading rubric

- Everything is good: 5 points
- Partially correct answers: 3-4 points
- Reasonable attempt: 2 points
## Question 1

Load the `palmerpenguins` package (already done above). Inspect the `penguins` tibble with `summary()` to see the distribution of variables and any missing values.

Use `drop_na()` to remove rows with `NA` values in the `penguins` tibble. Calculate how many rows were removed by subtracting the new count from the original count using `nrow()`.

Then, use `count()` to explore the data and see how many penguins of each species we have. This is a simple but powerful way to understand your data!

```{r}
summary(penguins)

# Remove rows with any missing values
penguins_nona <- drop_na(penguins)
nrow(penguins) - nrow(penguins_nona)

# Simple counting - this is a great way to explore data!
penguins |>
  count(species)

# You can count by multiple variables too
penguins |>
  count(species, island)
```

Then, use `replace_na()` to replace `NA` values in `bill_length_mm` and `bill_depth_mm` with a value of 0. You'll need to:

- Provide the data frame as the first argument
- Provide a named list showing which columns to replace and what values to use

```{r}
replace_na(penguins, list(bill_length_mm = 0, bill_depth_mm = 0))
```

## Question 2

Use `arrange`, `filter`, and `select` on a data frame. Let's build this step by step to understand how pipes work:

1. Import the data set `data/data_transcript_exp_tidy.csv` using `read_csv()` and `here()`.
2. **Step 2a**: First, just sort the tibble by expression data (`count`) from highest to lowest level using `arrange()`. Use `desc()` to get descending order.
3. **Step 2b**: Then add `filter()` to keep only rows where `count` > 100. Chain this with the pipe operator.
4. **Step 2c**: Finally, add `select()` to choose all columns *except* for `type`. Use the `-` operator to exclude columns.

```{r}
#| label: q2
exp_tbl <- read_csv(here("data/bootcamp/data_transcript_exp_tidy.csv.gz"))

# Step 2a: Just arrange first
exp_tbl |>
  arrange(desc(count)) # desc() for descending order

# Step 2b: Add the filter
exp_tbl |>
  arrange(desc(count)) |>
  filter(count > 100)

# Step 2c: Add the select (use -type to exclude the type column)
exp_tbl |>
  arrange(desc(count)) |>
  filter(count > 100) |>
  select(-type)
```

## Question 3

How will you:

1. create a new column `log10count` that contains log10 transformed `count` values using `mutate()` and `log10()` and
2. rearrange the columns in the following order: ensembl_transcript_id, type, time, replicate, count, log10count using `select()`.

Before showing the solution, remember:
- `mutate()` adds new columns (or modifies existing ones) - it keeps all existing columns
- `select()` chooses columns and can reorder them - list them in the order you want

```{r}
#| label: q3
exp_tbl |>
  mutate(log10count = log10(count)) |>
  select(ensembl_transcript_id, type, time, replicate, count, log10count)
```

## Question 4

Let's explore grouping operations step by step. We'll build your understanding progressively, starting with simple examples and then combining concepts.

**Step 4a**: First, try a simple grouping operation. Calculate the total count per transcript (ignoring time). Use:

- `group_by()` to group by transcript ID
- `summarize()` to calculate the sum of counts
- `.groups = "drop"` to remove grouping afterward (good practice!)

```{r}
#| label: q4a
# Simple grouping - sum counts for each transcript across all conditions
exp_tbl |>
  group_by(ensembl_transcript_id) |>
  summarize(
    total_count = sum(count),
    .groups = "drop" # This removes the grouping afterward - good practice!
  )
```

**Step 4b**: Now calculate a per-transcript sum, while keeping the `time` information (group by both transcript and time). This creates separate groups for each combination of transcript AND time:

```{r}
#| label: q4b
exp_tbl |>
  group_by(ensembl_transcript_id, time) |>
  summarize(
    count_sum = sum(count),
    .groups = "drop"
  )
```

## Question 5

Create meaningful categories from your data using `case_when()`. This function lets you create new variables based on multiple conditions

Categorize the expression levels in the count column into meaningful groups:

- "Low" for counts less than 50
- "Medium" for counts between 50 and 200 (inclusive of 50, exclusive of 200)
- "High" for counts between 200 and 1000 (inclusive of 200, exclusive of 1000)
- "Very High" for counts 1000 and above

Use `case_when()` inside `mutate()` to create a new column called expression_level, then use `count()` to see how many transcripts fall into each category.

```{r}
#| label: q5
# Categorize expression levels into meaningful groups
exp_tbl |>
  mutate(
    expression_level = case_when(
      count < 50 ~ "Low",
      count < 200 ~ "Medium",
      count < 1000 ~ "High",
      count >= 1000 ~ "Very High",
      .default = "undetermined" # in case of NA values
    )
  ) |>
  count(expression_level, sort = TRUE)
```
