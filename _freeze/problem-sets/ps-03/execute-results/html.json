{
  "hash": "3177da99534be940691a3b74405165c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R Bootcamp Problem Set 3\"\nauthor: \"Insert your name here\"\nexecute:\n  eval: false\n---\n\n## Setup\n\nStart by loading libraries you need analysis in the code chunk below.\nWhen in doubt, start by loading the tidyverse package.\n\n\n::: {.cell}\n\n:::\n\n\n## Problem Set\n\nEach problem below is worth **5 points**.\n\nUse the data files in the `data/` directory to answer the questions.\n\nFor this problem set, you are allowed to help each other, but you are not allowed to post correct answers in slack.\n\n**The problem set is due 5pm on Aug 28.**\n\n### Grading rubric\n\n- Everything is good: 5 points\n- Partially correct answers: 3-4 points\n- Reasonable attempt: 2 points\n\n## Question 1\n\nLoad the `palmerpenguins` package (already done above). Inspect the `penguins` tibble with `summary()` to see the distribution of variables and any missing values.\n\nUse `drop_na()` to remove rows with `NA` values in the `penguins` tibble. Calculate how many rows were removed by subtracting the new count from the original count using `nrow()`.\n\nThen, use `count()` to explore the data and see how many penguins of each species we have. This is a simple but powerful way to understand your data!\n\n\n::: {.cell}\n\n:::\n\n\nThen, use `replace_na()` to replace `NA` values in `bill_length_mm` and `bill_depth_mm` with a value of 0. You'll need to:\n\n- Provide the data frame as the first argument\n- Provide a named list showing which columns to replace and what values to use\n\n\n::: {.cell}\n\n:::\n\n\n## Question 2\n\nUse `arrange`, `filter`, and `select` on a data frame. Let's build this step by step to understand how pipes work:\n\n1. Import the data set `data/data_transcript_exp_tidy.csv` using `read_csv()` and `here()`.\n2. **Step 2a**: First, just sort the tibble by expression data (`count`) from highest to lowest level using `arrange()`. Use `desc()` to get descending order.\n3. **Step 2b**: Then add `filter()` to keep only rows where `count` > 100. Chain this with the pipe operator.\n4. **Step 2c**: Finally, add `select()` to choose all columns *except* for `type`. Use the `-` operator to exclude columns.\n\n\n::: {.cell}\n\n:::\n\n\n## Question 3\n\nHow will you:\n\n1. create a new column `log10count` that contains log10 transformed `count` values using `mutate()` and `log10()` and\n2. rearrange the columns in the following order: ensembl_transcript_id, type, time, replicate, count, log10count using `select()`.\n\nBefore showing the solution, remember:\n- `mutate()` adds new columns (or modifies existing ones) - it keeps all existing columns\n- `select()` chooses columns and can reorder them - list them in the order you want\n\n\n::: {.cell}\n\n:::\n\n\n## Question 4\n\nLet's explore grouping operations step by step. We'll build your understanding progressively, starting with simple examples and then combining concepts.\n\n**Step 4a**: First, try a simple grouping operation. Calculate the total count per transcript (ignoring time). Use:\n\n- `group_by()` to group by transcript ID\n- `summarize()` to calculate the sum of counts\n- `.groups = \"drop\"` to remove grouping afterward (good practice!)\n\n\n::: {.cell}\n\n:::\n\n\n**Step 4b**: Now calculate a per-transcript sum, while keeping the `time` information (group by both transcript and time). This creates separate groups for each combination of transcript AND time:\n\n\n::: {.cell}\n\n:::\n\n\n## Question 5\n\nCreate meaningful categories from your data using `case_when()`. This function lets you create new variables based on multiple conditions - it's like a more powerful version of `if_else()`.\n\nCategorize the expression levels in the `count` column into meaningful groups:\n- \"Low\" for counts less than 50\n- \"Medium\" for counts between 50 and 200 (inclusive of 50, exclusive of 200)\n- \"High\" for counts between 200 and 1000 (inclusive of 200, exclusive of 1000)\n- \"Very High\" for counts 1000 and above\n\nUse `case_when()` inside `mutate()` to create a new column called `expression_level`, then use `count()` to see how many transcripts fall into each category.\n\n\n::: {.cell}\n\n:::\n\n\n## Question 6\n\nTry to state and answer a new question! Stitch together several dplyr fuctions to answer a new question. I'm trying to wean you off the fill-in-the-blanks approach and get you to think independently using the tidyverse.\n\nHere are some ideas to get you started, you don't have to use any of them:\n\n- Which transcript has the highest expression level at each time point? (Hint: use `dplyr::slice_max()` after grouping by time)\n- What is the average expression level for each transcript across all time points? (Hint: use `group_by()` and `summarize()`)\n- Which time point has the highest total expression level across all transcripts? (Hint: group by time and summarize total count)\n\n\n::: {.cell}\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}