{
  "hash": "57ca91ea9aa16180950020ac7d7980a1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stats Bootcamp - class 13\"\nsubtitle: \"Hypothesis testing\"\nauthor: \"Neelanjan Mukherjee\"\neditor: visual\n---\n\n\n\n\n## Prepare mouse biochem data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we are reading the data directly from the internet\nbiochem <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/Biochemistry.txt\", show_col_types = FALSE) |>\n  janitor::clean_names()\n\n# simplify names a bit more\ncolnames(biochem) <- gsub(pattern = \"biochem_\", replacement = \"\", colnames(biochem))\n\n# we are going to simplify this a bit and only keep some columns\nkeep <- colnames(biochem)[c(1,6,9,14,15,24:28)]\nbiochem <- biochem[,keep]\n\n# get weights for each individual mouse\n# careful: did not come with column names\nweight <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/weight\", col_names = F, show_col_types = FALSE)\n\n# add column names\ncolnames(weight) <- c(\"subject_name\",\"weight\")\n\n# add weight to biochem table and get rid of NAs\n# rename gender to sex\nb <- inner_join(biochem, weight, by=\"subject_name\") |>\n  na.omit() |>\n  rename(sex=gender)\n```\n:::\n\n\n## Association between mouse $weight$ and $tot\\_cholesterol$ {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nggscatter()\n```\n:::\n\n\nwe previously established these are normal enough \\> $\\mathcal{H}_0$ is no (linear) relationship between $tot\\_cholesterol$ and $weight$\n\n------------------------------------------------------------------------\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nb |>\n  cor_test(??, ??,\n           method = \"??\"\n           )\n```\n:::\n\n\nP value well below 0.05\n\n> $\\mathcal{H}_0$ is no relationship between $tot\\_cholesterol$ and $weight$ **NOT WELL SUPPORTED**\n\nSo there is a (linear) relationship between $tot\\_cholesterol$ and $weight$\n\n## Visualize Pearson correlation {.smaller}\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggscatter(data = b,\n          y = \"weight\",\n          x = \"tot_cholesterol\"\n          ) +\n  stat_cor(method = \"pearson\",\n           label.x = 1,\n           label.y = 30)\n```\n\n::: {.cell-output-display}\n![](ex-13_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Manual calculation of Pearson correlation {.smaller}\n\n$Corr(x,y) = \\displaystyle \\frac {\\sum_{i=1}^{n} (x_{i} - \\overline{x})(y_{i} - \\overline{y})}{\\sum_{i=1}^{n} \\sqrt(x_{i} - \\overline{x})^2 \\sqrt(y_{i} - \\overline{y})^2}$\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# mean total cholesterol\nm_chol <-\n\n# average weight\nm_weight <-\n\n# difference from mean total cholesterol\ndiff_chol <-\n\n# difference from mean total weight\ndiff_weight <-\n\n# follow formula above\nmanual_pearson <-\n\nmanual_pearson\n```\n:::\n\n\n## Spearman Correlation (nonparametric) {.smaller}\n\n[Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)or Spearman's ρ, named after Charles Spearman is a **nonparametric** measure of **rank** correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.\n\nMore info [here](https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8#:~:text=The%20fundamental%20difference%20between%20the,with%20monotonic%20relationships%20as%20well.).\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nb |>\n  cor_test(weight, tot_cholesterol,\n           method = \"??\"\n           )\n```\n:::\n\n\nP value well below 0.05\n\n> $\\mathcal{H}_0$ is no relationship between $tot\\_cholesterol$ and $weight$ **NOT WELL SUPPORTED**\n\n## Visualize Spearman correlation\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggscatter(data = b,\n          y = \"weight\",\n          x = \"tot_cholesterol\"\n          ) +\n  stat_cor(method = \"spearman\",\n           label.x = 1,\n           label.y = 30)\n```\n\n::: {.cell-output-display}\n![](ex-13_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Let's create a hypothetical example {.smaller}\n\ncreate tibble $d$ with variables $x$ and $y$\n\n$x$, 1:50\n\n$y$, which is $x^{10}$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd <- tibble(\n  x=??,\n  y=??)\n```\n:::\n\n\n. . .\n\nscatter plot\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nggscatter(data = d,\n          x = \"x\",\n          y = \"y\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\nPearson\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd |>\n  cor_test(x, y,\n           method = \"pearson\"\n           ) |>\n  select(cor)\n```\n:::\n\n\n. . .\n\nSpearman\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd |>\n  cor_test(x, y,\n           method = \"spearman\"\n           ) |>\n  select(cor)\n```\n:::\n\n\n## Additional examples with correlation {.smaller}\n\ncompare 1 variable to all other quantitative variables\n\n$weight$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nb |>\n  cor_test(??) |>\n  gt()\n```\n:::\n\n\nrelationship between $weight$ and $tot\\_cholesterol$ by $sex$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nb |>\n\n  gt()\n```\n:::\n\n\n## Appropriate statistical test cheatsheet\n\n![](/img/stats_table.png)\n\n## Regression\n\nWe are going to change our frame work to learn about regression. The nice thing is that everything we learn for regression is applicable to all the tests we just learned.\n\n## The simplicity underlying common tests\n\nMost of the common statistical models (t-test, correlation, ANOVA; etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. It all comes down to:\n\n> $y = a \\cdot x + b$\n\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model.\n\n## Equation for a line {.smaller}\n\nRemember:\\\n$y = a \\cdot x + b$\\\nOR\\\n$y = b + a \\cdot x$\n\n$a$ is the **SLOPE** (2) $b$ is the **y-intercept** (1)\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd <- tibble(x=c(-1,3),\n            y=c(-1,6)\n             )\n\nggplot(data=d, aes(x=x,y=y)) +\n  geom_blank() +\n  geom_abline(intercept = 1,\n              slope = 2,\n              col = \"red\") +\n  theme_linedraw()\n```\n\n::: {.cell-output-display}\n![](ex-13_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Stats equation for a line {.smaller}\n\nModel:\n\n$y$ equals the intercept ($\\beta_0$) pluss a slope ($\\beta_1$) times $x$.\n\n$y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0$\n\n... which is the same as $y = b + a \\cdot x$.\n\nThe short hand for this in R: `y ~ 1 + x`\n\nR interprets this as:\n\n`y = 1*number + x*othernumber`\n\nThe task of t-tests, lm, etc., is simply to find the numbers that best predict $y$.\n\n## Stats equation for a line {.smaller}\n\nAll you need is an intercept ($\\beta_0$) and a slope ($\\beta_1$) to get a line:\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nggplot(data=d, aes(x=x,y=y)) +\n  geom_blank() +\n  geom_abline(intercept = 1,\n              slope = 2,\n              col = \"red\") +\n  theme_linedraw()\n```\n\n::: {.cell-output-display}\n![](ex-13_files/figure-html/unnamed-chunk-16-1.png){width=192}\n:::\n:::\n\n\n$\\beta_0$ = 1 (the y-intercept), $\\beta_1$ = 2 (the slope)\n\n$y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x$\\\n\n$y = 1 \\cdot 1 + 2 \\cdot x$\n\n$y = 1 + 2x$\n\n> Our mission: **FIND THE BEST** $\\beta$ coefficients\n\n## Linear Regression\n\n-   STEP 1: Make a scatter plot visualize the linear relationship between x and y.\n-   STEP 2: Perform the regression\n-   STEP 3: Look at the $R^2$, $F$-value and $p$-value\n-   STEP 4: Visualize fit and errors\n-   STEP 5: Calculate $R^2$, $F$-value and $p$-value ourselves\n\n## STEP 1: Can mouse cholesterol levels explain mouse weight?\n\nPlot $weight$ (y, response variable) and $tot_cholesterol$ (x, explanatory variable)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ??,\n       aes(y = ??,\n           x = ??)) +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_linedraw()\n```\n:::\n\n\n## STEP 2: Do the regression {.smaller}\n\nKeep calm and fit a line! Remember: $y = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x$\n\nlinear model equation: $weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot tot\\_cholesterol$\n\n> $\\mathcal{H}_0:$ $tot\\_cholesterol$ does NOT explain $weight$ Null Hypothesis: $\\mathcal{H}_0: \\beta_1 = 0$\n\n$weight = \\beta_0 \\cdot 1 + 0 \\cdot tot\\_cholesterol$ $weight = \\beta_0 \\cdot 1$\n\n> $\\mathcal{H}_1:$ Mouse $tot\\_cholesterol$ does explain $weight$\n\n$weight = \\beta_0 \\cdot 1 + \\beta_1 \\cdot tot\\_cholesterol$\n\nThe cool thing here is that we can assess and compare our null and alternative hypothesis by learning and examining the model coefficients (namely the slope). Ultimately, we are comparing a complex model (with cholesterol) to a simple model (without cholesterol).\n\n<https://statisticsbyjim.com/regression/interpret-constant-y-intercept-regression/>\n\n## STEP 4: Look at the $R^2$, $F$-value and $p$-value {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n# fitting a line\nfit_WvC <- lm(\n  data = ??,\n  formula = ??)\n\n\n# base R summary of fit\nsummary(fit_WvC)\n```\n:::\n\n\nThat's a lot of info, but how would I access it? Time to meet your new best friend:\n\n[Broom](%22https://cran.r-project.org/web/packages/broom/vignettes/broom.html%22)\n\n## Tidying output {.smaller}\n\ninformation about the model fit\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n??(fit_WvC) |>\n  gt() |>\n  fmt_number(columns = r.squared:statistic, decimals = 3)\n```\n:::\n\n\ninformation about the intercept and coefficients\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n??(fit_WvC)|>\n  gt() |>\n  fmt_number(columns =estimate:statistic, decimals = 3)\n```\n:::\n\n\nsave the intercept and slope into variable to use later\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nchol_intercept <-\n\nchol_slope <-\n```\n:::\n\n\n------------------------------------------------------------------------\n\n> for every 1 unit increase in cholesterol there is a 1.85 unit increase weight\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = b,\n       aes(y = weight,\n           x = tot_cholesterol)) +\n  geom_smooth(method = \"lm\") +\n  geom_point(size=.5) +\n  scale_color_manual() +\n  theme_linedraw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ex-13_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Collecting residuals and other information {.smaller}\n\nadd residuals and other information\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nb_WvC <- ??(fit_WvC, data = b)\n\nb_WvC\n```\n:::\n\n\n## What are **Residuals** {.smaller}\n\n**Residuals**, $e$ --- the difference between the observed value of the response variable $y$ and the explanatory value $\\widehat{y}$ is called the residual. Each data point has one residual. Specifically, it is the distance on the y-axis between the observed $y_{i}$ and the fit line.\n\n$e = y_{i} - \\widehat{y}$\n\nResiduals with large absolute values indicate the data point is NOT well explained by the model.\n\n## STEP 5: Visualize fit and errors\n\nVisualize the residuals OR the error around the fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = b_WvC,\n       aes(x = tot_cholesterol, y = weight)) +\n  geom_point(size=1, aes(color = .resid)) +\n  geom_abline(intercept = pull(chol_intercept),\n              slope = pull(chol_slope),\n              col = \"red\") +\n  scale_color_gradient2(low = \"blue\",\n                        mid = \"black\",\n                        high = \"yellow\") +\n  geom_segment(aes(xend = tot_cholesterol,\n                   yend = .fitted),\n               alpha = .1) + # plot line representing residuals\n  theme_linedraw()\n```\n:::\n\n\n------------------------------------------------------------------------\n\nVisualize the total error OR the error around the null. So no cholesterol fit, just the mean of y.\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_weight <- mean(b_WvC$weight)\n```\n:::\n\n\n## STEP 6: Calculate $R^2$, $F$-value, $p$-value ourselves {.smaller}\n\n## What is $R^2$ {.smaller}\n\n$R^2$ --- the coefficient of determination, which is the proportion of the variance in the response variable that is predictable from the explanatory variable(s).\n\n$R^2 = 1 - \\displaystyle \\frac {SS_{fit}}{SS_{null}}$\n\n$SS_{fit}$ --- sum of squared errors around the least-squares fit\n\n$SS_{fit} = \\sum_{i=1}^{n} (data - line)^2 = \\sum_{i=1}^{n} (y_{i} - (\\beta_0 \\cdot 1+ \\beta_1 \\cdot x)^2$\n\n$SS_{null}$ --- sum of squared errors around the mean of $y$\n\n$SS_{null} = \\sum_{i=1}^{n} (data - mean)^2 = \\sum_{i=1}^{n} (y_{i} - \\overline{y})^2$\n\n## Calculate $R^2$ {.smaller}\n\n$SS_{fit}$ --- sum of squared errors around the least-squares fit\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nss_fit <- ??\nss_fit\n```\n:::\n\n\n$SS_{null}$ --- sum of squared errors around the mean of $y$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nss_null <- ??\nss_null\n```\n:::\n\n\n$R^2 = 1 - \\displaystyle \\frac {SS_{fit}}{SS_{null}}$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nrsq <- 1- ??\n\nglance(fit_WvC) |> select(r.squared)\n```\n:::\n\n\n. . .\n\nBTW this is the same $R$ as from the Pearson correlation, just squared:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb |>\n  cor_test(weight,tot_cholesterol,\n           method = \"pearson\") |>\n  mutate(r2 = cor^2) |>\n  pull(r2) |>\n  round(2)\n```\n:::\n\n\n## Interpret $R^2$\n\nThere is a 13 % reduction in the variance when we take mouse $cholesterol$ into account\\\nOR\\\n$cholesterol$ explains 13% of variation in mouse $weight$\n\n## What is the **F-statistic** {.smaller}\n\n**F-statistic** --- based on the ratio of two variances: the explained variance (due to the model) and the unexplained variance (residuals).\n\n$F = \\displaystyle \\frac{SS_{fit}/(p_{fit}-p_{null})} {SS_{null}/(n-p_{fit})}$\n\n$p_{fit}$ --- number of parameters (coefficients) in the fit line\\\n\n$p_{null}$ --- number of parameters (coefficients) in the mean line\\\n\n$n$ --- number of data points\n\n## Calculate the **F-statistic** {.smaller}\n\n$F = \\displaystyle \\frac{SS_{null} - SS_{fit}/(p_{fit}-p_{null})} {SS_{fit}/(n-p_{fit})}$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\npfit <- ??\npnull <- ??\nn <- ??\n\nf <- ??\n\n\nf\n\nglance(fit_WvC) |> select(statistic)\n```\n:::\n\n\n## P-values {.smaller}\n\nYou don't really need to know what the $F-statistic$ is unless you want to calculate the p-value. In this case we need to generate a null distribution of $F-statistic$ values to compare to our observed $F-statistic$.\n\nTherefore, we will randomize the $tot_cholesterol$ and $weight$ and then calculate the $F-statistic$.\n\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"10%\"}\n\n</br>\n\nWe will do this many many times to generate a null distribution of $F-statistic$s.\\\n</br>\n\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"} ![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"}\\\n![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"} ![](https://media.giphy.com/media/3o7TKDedZiHXRJNveU/giphy.gif){width=\"5%\"} </br>\n\nThe p-value will be the probability of obtaining an $F-statistic$ in the null distribution at least as extreme as our observed $F-statistic$.\n\n## Let's get started\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# set up an empty tibble to hold our null distribution\nfake_biochem <- tribble()\n\n# we will perform 100 permutations\nmyPerms <- 100\n\nfor (i in 1:myPerms) {\n  tmp <- bind_cols(\n    b_WvC[sample(nrow(b_WvC)), \"weight\"],\n    b_WvC[sample(nrow(b_WvC)),\"tot_cholesterol\"],\n    \"perm\"=factor(rep(i,nrow(b_WvC)))\n    )\n\n  fake_biochem <- bind_rows(fake_biochem,tmp)\n  rm(tmp)\n\n}\n\n\n# let's look at permutations 1 and 2\nggplot(fake_biochem |> filter(perm %in% c(1:2)), aes(x=weight, y=tot_cholesterol, color=perm)) +\n  geom_point(size=.1) +\n  theme_minimal()\n```\n:::\n\n\n## Run 100 linear models!\n\nNow we will calculate and extract linear model results for each permutation individually using nest, mutate, and map functions\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nfake_biochem_lms <- fake_biochem |>\n  nest(data = -perm) |>\n  mutate(\n    fit = map(data, ~ lm(weight ~ tot_cholesterol, data = .x)),\n    glanced = map(fit, glance)\n  ) |>\n  unnest(glanced)\n```\n:::\n\n\n## Visualize the null {.smaller}\n\nLet's take a look at the null distribution of F-statistics from the randomized values\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nggplot(fake_biochem_lms,\n       aes(x = statistic)) +\ngeom_density(color=\"red\") +\ntheme_minimal()\n```\n:::\n\n\n------------------------------------------------------------------------\n\nremember that the $F-statistic$ we observed was 255!\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(fake_biochem_lms, aes(x = statistic)) +\nxlim(0,f*1.1) +\ngeom_density(color=\"red\") +\ngeom_vline(xintercept = f, color = \"blue\") +\n#  scale_x_log10() +\ntheme_minimal()\n```\n:::\n\n\nIn our 100 randoized simulations, we never see an F-statistic as extreme as the one we observed in the actual data. Therefore:\n\n> P \\< 0.01 or 1/100\n\nReminder, our calculate P value was:\n\n\n\n## How to find the best (least squares) fit?\n\n1.  Rotate the line of fit\\\n2.  Find the fit that minimizes the Sum of Squared Residuals or $SS_{fit}$\\\n3.  This is the derivative (slope of tangent at best point = 0) of the function describing the $SS_{fit}$ and the next rotation is 0.\n\n## References\n\n[Differences between correlation and regression](https://www.graphpad.com/support/faq/what-is-the-difference-between-correlation-and-linear-regression/)\n\n[also more differences between correlation and regression](https://keydifferences.com/difference-between-correlation-and-regression.html).\n\n[Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/) from Jonas Lindeløv\\\n\n[Statquest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)\n\n[Stats gobbledygook](https://www.rapidtables.com/math/symbols/Statistical_Symbols.html)\n\n[Linear Regression Assumptions and Diagnostics in R: Essentials](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)\\\n\n[PRINCIPLES OF STATISTICS](https://www.graphpad.com/guides/prism/latest/statistics/stat_---_principles_of_statistics_-.htm) from GraphPad/SAS.\n\n[Statquest: how to go from F-statistic to p-value](https://www.youtube.com/watch?v=nk2CQITm_eo)\n\n[StatQuest: Fitting a line to data, aka least squares, aka linear regression.](https://youtu.be/PaFPbb66DxQ)\n\n[StatQuest: Gradient Descent, Step-by-Step](https://youtu.be/sDv4f4s2SB8)\n",
    "supporting": [
      "ex-13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}