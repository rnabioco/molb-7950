{
  "hash": "2702270aef2405f6646b1575619b47ca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stats Bootcamp - class 14\"\nsubtitle: \"Hypothesis testing\"\nauthor: \"Neelanjan Mukherjee\"\neditor: visual\n---\n\n\n\n## Prepare mouse biochem data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we are reading the data directly from the internet\nbiochem <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/Biochemistry.txt\", show_col_types = FALSE) |>\n  janitor::clean_names()\n\n# simplify names a bit more\ncolnames(biochem) <- gsub(pattern = \"biochem_\", replacement = \"\", colnames(biochem))\n\n# we are going to simplify this a bit and only keep some columns\nkeep <- colnames(biochem)[c(1,6,9,14,15,24:28)]\nbiochem <- biochem[,keep]\n\n# get weights for each individual mouse\n# careful: did not come with column names\nweight <- read_tsv(\"http://mtweb.cs.ucl.ac.uk/HSMICE/PHENOTYPES/weight\", col_names = F, show_col_types = FALSE)\n\n# add column names\ncolnames(weight) <- c(\"subject_name\",\"weight\")\n\n# add weight to biochem table and get rid of NAs\n# rename sex to sex\nb <- inner_join(biochem, weight, by=\"subject_name\") |>\n  na.omit() |>\n  rename(sex=gender)\n```\n:::\n\n\n## Learning objectives\n\n-   **Formulate** and **Execute** null hypothesis testing\n-   **Identify** and **Perform** the proper statistical test for data type/comparison\n-   **Calculate** and **Interpret** p-values\n\n## Random variables\n\n**Response Variable** ( **y** - aka dependent or outcome variable): this variable is predicted or its variation is explained by the explanatory variable. In an experiment, this is the outcome that is measured following manipulation of the explanatory variable.\n\n**Explanatory Variable** ( **x** - aka independent or predictor variable): explains variations in the response variable. In an experiment, it is manipulated by the researcher.\n\n## The simplicity underlying common tests\n\nMost of the common statistical models (t-test, correlation, ANOVA; etc.) are special cases of linear models or a very close approximation. This simplicity means that there is less to learn. It all comes down to:\n\n> $y = a \\cdot x + b$\n\nThis needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model.\n\n## Stats equation for a line {.smaller}\n\nModel:\n\n$y$ equals the intercept ($\\beta_0$) pluss a slope ($\\beta_1$) times $x$.\n\n$y = \\beta_0 + \\beta_1 x \\qquad \\qquad \\mathcal{H}_0: \\beta_1 = 0$\n\n... which is the same as $y = b + a \\cdot x$.\n\nThe short hand for this in R: `y ~ 1 + x`\n\nR interprets this as:\n\n`y = 1*number + x*othernumber`\n\nThe task of t-tests, lm, etc., is simply to find the numbers that best predict $y$.\n\n## Appropriate statistical test cheatsheet\n\n![](/img/stats_table.png)\n\n## Comparing means between two groups {.smaller}\n\nWe will compare mouse $weight$ by $sex$.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# plot weight by sex\np_ws <- ggplot(b,\n               aes(x = sex, y = weight)) +\n  geom_jitter(size=1) +\n  geom_hline(yintercept = mean(b$weight),\n             color = \"red\") +\n  theme_cowplot()\n\n\n# plot weight by sex with mean weight and mean weight by sex\np_ws2 <- ggplot(b,\n                aes(x = sex, y = weight)) +\n  geom_jitter(size=1) +\n  geom_hline(yintercept = mean(b$weight),\n             color = \"red\") +\n stat_summary(fun = \"mean\", geom = \"point\",  fill = \"blue\", shape = 23, size=3) +\n  theme_cowplot()\n\nplot_grid(p_ws, p_ws2, ncol = 2, labels = c(\"weight\",\"weight by sex\"), scale = c(1,1))\n```\n\n::: {.cell-output-display}\n![](ex-14_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## STEP 1: Can mouse sex explain mouse weight? {.smaller}\n\nModel: $y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}$\n\nNull Hypothesis: $\\mathcal{H}_0: \\beta_1 = 0$\n\n$\\mathcal{H}_0:$ mouse $sex$ does NOT explain $weight$\n\nAlternative Hypothesis: $\\mathcal{H}_1: \\beta_1 \\neq 0$\n\n$\\mathcal{H}_1:$ mouse $sex$ does explain $weight$\n\n**Important:** $x_{i}$ is an indicator (0 or 1) saying whether data point i was sampled from one or the other group (female or male).\n\nWe will explore this in more detail soon.\n\n## STEP 2: Fit linear model and examine results {.smaller}\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nfit_ws <-\n```\n:::\n\n\nFit summary:\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n??(fit_ws) |>\n  gt() |>\n  fmt_number(columns = r.squared:statistic, decimals = 3)\n```\n:::\n\n\nCoefficient summary:\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n??(fit_ws) |>\n  gt() |>\n  fmt_number(columns =estimate:statistic, decimals = 3)\n```\n:::\n\n\n## Collecting residuals and other information {.smaller}\n\nadd residuals and other information\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n#augment\nb_ws <-\n```\n:::\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n# mean weight\navg_w <-\n\n# mean weight female\navg_wf <-\n\n\n# mean weight male\navg_wm <-\n```\n:::\n\n\n## STEP 3: Visualize the error around fit {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n# plot of data with mean and colored by residuals\n\np_ws <- ggplot(b_ws,\n               aes(x = sex, y = weight)) +\n  geom_point(position = position_jitter(),\n             aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\",\n                        mid = \"black\",\n                        high = \"yellow\") +\n    geom_hline(yintercept = ??,\n               color = \"darkgrey\") +\n  geom_segment(aes(x=.5, xend=1.5,\n                   y=??, yend=??),\n               color=\"red\") +\n    geom_segment(aes(x=1.5, xend=2.5,\n                     y=??), yend=??,\n                 color=\"red\") +\n  theme_cowplot()\n\np_ws\n```\n:::\n\n\n## STEP 3: Visualize the error around the null (mean weight) {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\np_w <- ggplot(b_ws,\n               aes(x = sex, y = weight)) +\n  geom_point(position = position_jitter(),\n             aes(color = weight-avg_w)) +\n  scale_color_gradient2(low = \"blue\",\n                        mid = \"black\",\n                        high = \"yellow\") +\n    geom_hline(yintercept = avg_w,\n               color = \"darkgrey\") +\n  theme_cowplot()\n\np_w\n```\n:::\n\n\n## Compare fit error to null error graphically {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(p_ws, p_w, ncol = 2, labels = c(\"weight by sex\",\"weight by intercept\"))\n```\n:::\n\n\nWe are fitting 2 lines to the data. For the weight by sex model of the fit (left), we fit **2** lines. For the weight by null model (right) we fit **1** line.\n\n## Exceptions: mice with highest residuals {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb_ws |>\n  arrange(??) |>\n  top_n(15) |>\n  select(subject_name,weight,sex,.resid,.fitted) |>\n  gt() |>\n  fmt_number(decimals = 2)\n```\n:::\n\n\n## Matrices Interlude *Begin* {.smaller}\n\n> How do we go from **2 fit lines** to **1 equation**\n\nSince we don't want to calculate any of this by hand, the framework needs to be flexible such that a computer can execute for different flavors of comparison (cont y vs cont x, cont y vs 2 or more categorical x, ...).\n\n## Let's focus on just a few mice {.smaller}\n\nRemember that:\\\n$weight$ is $y$\\\n$F_{avg}$ is the average $weight$ of $females$\\\n$M_{avg}$ is the average $weight$ of $males$\n\n. . .\n\nA0480548**85**, female\\\n$y_{85}= 1 \\cdot F_{avg} + 0 \\cdot M_{avg} + residual_{85}$\n\nA0671097**71**, female\\\n$y_{71}= 1 \\cdot F_{avg} + 0 \\cdot M_{avg} + residual_{71}$\n\n. . .\n\nA0668223**51**, male\\\n$y_{51}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{51}$\n\nA0482743**62**, male\\\n$y_{62}= 0 \\cdot F_{avg} + 1 \\cdot M_{avg} + residual_{62}$\n\n## Let's focus on just a few mice {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb_ws |>\n  filter(subject_name %in% c(\"A048054885\",\"A067109771\",\"A066822351\",\"A048274362\")) |>\n  select(subject_name, weight, sex, .fitted, .resid) |>\n  arrange(sex) |>\n  gt()\n```\n:::\n\n\n## Need a volunteer {.smaller}\n\n**Me:** Ooohh my, imagine how tedious it would be to do this for all mice...\\\n**Volunteer:** Wait a sec...isn't there a way to formulate this as a matrix algebra problem.\\\n**Me:** You're right - I'm so glad you asked! Let's conjur matrix-magic to solve this problem..\n\n. . .\n\n$f_{avg} = \\beta_0$ is the average $weight$ of $female$ mice\\\n$m_{avg} = \\beta_1$ is the average $weight$ of $male$ mice\n\n. . .\n\n$\\begin{bmatrix} y_{85} \\\\ y_{71} \\\\ y_{51} \\\\y_{62} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{85} \\\\ e_{71} \\\\ e_{51} \\\\e_{62} \\end{bmatrix}$\n\n. . .\n\n**So basically this looks like the same equation for fitting a line we've been discussing, just w/a few more dimensions :)**\n\nThis is a conceptual peak into the underbelly of how the $\\beta$ cofficients and least squares is performed using matrix operations (linear algebra). If you are interested in learning more see references at the end of the slides.\n\nMatrices Interlude *FIN*\n\n## Calculate $R^2$ {.smaller}\n\n$SS_{fit}$ --- sum of squared errors around the least-squares fit\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nss_fit <-\n```\n:::\n\n\n$SS_{null}$ --- sum of squared errors around the mean of $y$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nss_null <-\n```\n:::\n\n\n$R^2 = 1 - \\displaystyle \\frac {SS_{fit}}{SS_{null}}$\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nrsq <- ??\n```\n:::\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nglance(fit_ws) |> select(r.squared)\n```\n:::\n\n\nWoohoo!!\n\n## Compare to traditional t-test {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nb |>\n  ?? |>\n  select(-c(n1,n2,df)) |>\n  gt()\n```\n:::\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\ntidy(fit_ws) |>\n  select(term, estimate, statistic, p.value) |>\n  gt()\n```\n:::\n\n\n## prep data for fams {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# i have pre-selected some families to compare\nmyfams <- c(\"B1.5:E1.4(4) B1.5:A1.4(5)\",\n            \"F1.3:A1.2(3) F1.3:E2.2(3)\",\n            \"A1.3:D1.2(3) A1.3:H1.2(3)\",\n            \"D5.4:G2.3(4) D5.4:C4.3(4)\")\n\n# only keep the familys in myfams\nbfam <- b |>\n  filter(family %in% myfams) |>\n  droplevels()\n\n# simplify family names and make factor\nbfam$family <- gsub(pattern = \"\\\\..*\", replacement = \"\", x = bfam$family) |>\n  as.factor()\n\n\n# make B1 the reference (most similar to overall mean)\nbfam$family <- relevel(x = bfam$family, ref =\"B1\")\n```\n:::\n\n\n## STEP 1: Can family explain weight? {.smaller}\n\nANOVA -\\> comparing means of 3 or more groups.\n\nLet's compare the $weight$ by $family$, but only for a few selected families.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(data = ??,\n       aes(??)) +\n  geom_jitter(width = .2) +\n  theme_cowplot()\n```\n:::\n\n\n## What does the model look like? {.smaller}\n\nModel: $y_{i} = \\beta_0 \\cdot 1+ \\beta_1 \\cdot x_{i}$\n\nNull Hypothesis: $\\mathcal{H}_0: \\beta_1 = 0$\n\n$\\mathcal{H}_0:$ mouse $family$ does NOT explain $weight$\n\nAlternative Hypothesis: $\\mathcal{H}_1: \\beta_1 \\neq 0$\n\n$\\mathcal{H}_1:$ mouse $family$ does explain $weight$\n\n**Important:** $x_{i}$ is an indicator (0 or 1) saying which group point $i$ was sampled from using the matrix encoding of 0s and 1s.\n\nBelow is an example depicting 6 observations with 2 from each of 3 families:\n\n$\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\\\y_{4} \\\\y_{5} \\\\y_{5} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ e_{3} \\\\e_{4} \\\\e_{5} \\\\e_{6} \\end{bmatrix}$\n\n## STEP 2: Fit linear model and examine results {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nfit_wf <- ??\n```\n:::\n\n\nFit summary:\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nglance(fit_wf) |>\n  gt() |>\n  fmt_number(columns = r.squared:statistic, decimals = 3)\n```\n:::\n\n\nCoefficient summary:\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\ntidy(fit_wf) |>\n  gt() |>\n  fmt_number(columns =estimate:statistic, decimals = 3)\n```\n:::\n\n\n## Collecting residuals and other information {.smaller}\n\nadd residuals and other information\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n#augment\nb_wf <-\n\n# mean weight per fam\nmean_B1 <- fit_wf$coefficients[??]\n\nmean_A1 <- mean_B1 +\n  fit_wf$coefficients[??]\n\nmean_D5 <- mean_B1 +\n  fit_wf$coefficients[??]\n\nmean_F1 <- mean_B1 +\n  fit_wf$coefficients[??]\n```\n:::\n\n\n## STEP 3: Visualize the error around fit {.smaller}\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(b_wf,\n       aes(x = family, y = weight)) +\n  geom_point(position = position_jitter(),\n             aes(color = .resid)) +\n  scale_color_gradient2(low = \"blue\",\n                        mid = \"black\",\n                        high = \"yellow\") +\ngeom_segment(aes(x=.5, xend=1.5, y=mean_B1, yend=mean_B1), color=\"red\") +\n  geom_segment(aes(x=1.5, xend=2.5, y=mean_A1, yend=mean_A1), color=\"red\") +\n  geom_segment(aes(x=2.5, xend=3.5, y=mean_D5, yend=mean_D5), color=\"red\") +\n  geom_segment(aes(x=3.5, xend=4.5, y=mean_F1, yend=mean_F1), color=\"red\") +\n  geom_segment(aes(x=.5, xend=4.5, y=mean(weight), yend=mean(weight)), color=\"black\") +\n  theme_cowplot()\n```\n:::\n\n\n## Compare to traditional ANOVA {.smaller}\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nbfam |>\n  ?? |>\n  gt()\n```\n:::\n\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\ntidy(fit_wf) |>\n  select(term, estimate, statistic, p.value) |>\n  gt()\n```\n:::\n\n\n## Comparing 2 groups of 2 continuous variables {.smaller}\n\n**ANCOVA, Analysis of Covariance.** ANOVA with more than one independent variable. What is the impact of mouse age on mouse weight for males vs females.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(data = b, aes(y = weight, x = age, color=sex)) +\n  geom_point(size=.5) +\n  geom_smooth(method=lm) +\n  theme_cowplot()\n```\n:::\n\n\n## STEP 2: Fit linear model and examine results {.smaller}\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nfit_wa_sex <- lm(formula = weight ~ 1 + age + sex, data = b)\nb_wa_sex <- augment(fit_ws, data = b)\n```\n:::\n\n\nFit summary:\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nglance(fit_wa_sex) |>\n  gt() |>\n  fmt_number(columns = r.squared:statistic, decimals = 3)\n```\n:::\n\n\nCompare to traditional:\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\naov(formula = weight ~ 1 + age + sex, data = b) |>\n  glance()\n```\n:::\n\n\n## References\n\n[Linear Models Pt.3 - Design Matrices](https://www.youtube.com/watch?v=CqLGvwi-5Pc&list=PLblh5JKOoLUIzaEkCLIUxQFjPIlapw8nU&index=6)\\\n\n[A Matrix Formulation of the Multiple Regression Model](https://online.stat.psu.edu/stat462/node/132/)\n\n[Doing and reporting your first ANOVA and ANCOVA in R](https://towardsdatascience.com/doing-and-reporting-your-first-anova-and-ancova-in-r-1d8209)40f2ef\n",
    "supporting": [
      "ex-14_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}