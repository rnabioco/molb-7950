{
  "hash": "09bd8fa464d9aeaa16f526003ea5239b",
  "result": {
    "markdown": "---\ntitle: \"Stats Bootcamp - class 15\"\nsubtitle: \"Dealing with big data\"\nauthor: \"Neelanjan Mukherjee\"\neditor: visual\n---\n\n\n\n\n## Learning objectives\n\n-   Types of error and multiple test corrections\n-   Exploratory data analysis\n-   Clustering and overlaps\n\n## Types I and II error\n\nFalse positives and False negatives\n\n![](https://www.scribbr.com/wp-content/uploads/2021/01/type-i-and-ii-error-2.png)\n\n$\\alpha$ - significance level OR evidentiary standard\n\n$\\beta$ - type II error rate, 1 - $\\beta$ is power\n\n## Different visualization\n\nPower vs Significance\n\n![](https://www.scribbr.com/wp-content/uploads/2021/01/type-i-and-type-ii-error.png)\n\n## Genomics -\\> Lots of Data -\\> Lots of Hypothesis Tests\n\nIn a typical RNA-seq experiment, we test \\~10K different hypotheses. For example, you have 10K genes and for each gene you test whether the mean expression changed in condition A vs condition B. Using a standard p-value cut-off of 0.05, we'd expect **500 genes** to be deemed \"significant\" by chance. Thus, we are very concerned about **False Positives or Type I Errors**.\n\n## Multiple test corrections {.smaller}\n\n1.  Control overall <ce><b1> (also known as family-wise error rate or [FWER](https://en.wikipedia.org/wiki/Family-wise_error_rate)), which will affect the <ce><b1>\\* for each test. That is, we are controlling the overall probability of making *at least one* false discovery. Bonferroni and Sidak corrections all control FWER.\n\n2.  Control [false discovery rate](https://en.wikipedia.org/wiki/False_discovery_rate) (FDR). These procedures allow for type 1 errors (false positives) but control the proportion of these false positives in relation to true positives. This is done by adjusting the decision made for the p-value associated with each individual test to decide rejection or not. Because this will result in a higher type 1 error rate, it has higher [power](https://en.wikipedia.org/wiki/Power_(statistics)). This affords a higher probability of *true discoveries.* The step procedures control for FDR.\n\n## Bonferroni Correction {.smaller}\n\nThe most conservative of corrections, the Bonferroni correction is also perhaps the most straightforward in its approach. Simply divide <ce><b1> by the number of tests (*m*).\n\n> <ce><b1> = <ce><b1>/m\n\nHowever, with many tests, <ce><b1> will become very small. This reduces power, which means that we are very unlikely to make any true discoveries.\n\n### Sidak Correction\n\n> <ce><b1> = 1-(1-<ce><b1>)\\^(1/*m*)\n\n## Holm's Step-Down Procedure {.smaller}\n\nThe Holm-Bonferroni method is also fairly simple to calculate, but it is more powerful than the single-step Bonferroni.\n\n$HB = \\displaystyle \\frac {target \\alpha}{n - rank + 1}$\n\nH1: 0.005\\\nH2: 0.01\\\nH3: 0.03\\\nH4: = 0.04\\\n\nStep 1: Order the p-values from smallest to greatest (already done)\n\nStep 2: Calc HB for the first rank HB = .05 / 4 -- 1 + 1 = .05 / 4 = .0125 H1: 0.005 \\< .0125, so we reject the null\n\nStep 4: Repeat the HB formula for the second rank and keep going until we find $H{_N}$ \\> $HB{_N}$. All subsequent hypotheses are non-significant (i.e. not rejected).\n\n## Hochberg's Step-Up Procedure {.smaller}\n\nMore powerful than Holm's step-down procedure, Hochberg's step-up procedure also seeks to control the FDR and follows a similar process, only p-values are ranked from largest to smallest.\n\nFor each ranked p-value, it is compared to the <ce><b1> calculated for its respective rank (same formula as Holm's procedure). Testing continues until you reach the first non-rejected hypothesis. You would then fail to reject all following hypotheses.\n\n## Example {.smaller}\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nrna <- read_csv(here(\"data/data_rna_protein.csv.gz\")) |>select(iDUX4_pval)\n\n\nrna$fdr <- p.adjust(p = ??, method = \"fdr\", n = nrow(rna))\n\nrna$BH <- p.adjust(p = rna$iDUX4_pval, method = \"BH\", n = nrow(rna))\n\nrna$bon <- p.adjust(p = rna$iDUX4_pval, method = \"bonferroni\", n = nrow(rna))\n\n\nrna_long <- rna |>pivot_longer(cols = iDUX4_pval:bon, names_to = \"type\") \n\n\n\nggplot(data = rna_long, aes(x=value, fill = type)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(~type) +\n  theme_cowplot()\n```\n:::\n\n\n## Exploratory data analysis (EDA) {.smaller}\n\nOur goal here is to get an top-down big picture of the similarity/differences between variables in a dataset. For example, let's say you do RNA-seq in triplicate on 4 treatment/developmental times.\n\n### PCA\n\nWe will perform PCA on all of the samples and visualize the relationship between samples.\n\n### Correlation matrix\n\nWe will perform hierarchical clustering on a matrix representing the pairwise correlation between all these samples.\n\n## Explore data {.smaller}\n\nIs it normal-ish?\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n# get dux targets\ndux_targets <- read_csv(file = here(\"data\",\"target_genes.csv.gz\"))\n\n# get expression data\nd <- read_tsv(here(\"data\",\"data_genelevel.tsv.gz\")) |>\n  mutate(target = case_when(\n    gene_symbol %in% dux_targets$hgnc_symbol ~ \"target\",\n    TRUE ~ \"not_target\")\n         ) |>\n  filter(gene_symbol!=\"ISCA1\") |>\n  drop_na()\n\n\nd |>\n  pivot_longer(cols = hour00_rep1:hour14_rep3) |> \n  ggplot(aes(x=value, color=name)) +\n  ??() +\n  theme_cowplot()\n```\n:::\n\n\n. . .\n\nDefinitely not normal\n\n## Data transformations {.smaller}\n\nWe often transform data to make it closer to being normally-distributed. This allows us to use more powerful statistical tests on the same data. One approach is to log-transform the data.\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd |>\n  pivot_longer(cols = hour00_rep1:hour14_rep3) |> \n  ggplot(aes(x=??(value), color=name)) +\n  geom_density() +\n  theme_cowplot()\n```\n:::\n\n\n. . .\n\nWhat is this?\n\n> Warning message: Removed 1251 rows containing non-finite values (stat_density()).\n\n## Pseudocounts {.smaller}\n\n$log_{x}(0)$ is a common problem. One solution is to add a pseudocount. Since this is read count data, the smallest unit is 1 and so we will add 1 to all the observations before perforing the log transformation. $1$ represents the pseudocount in this case.\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\nd |>\n  pivot_longer(cols = hour00_rep1:hour14_rep3) |> \n  ggplot(aes(x=??(value), color=name)) +\n  geom_density() +\n  theme_cowplot()\n```\n:::\n\n\n## correlation analysis\n\nprepare the data for analysis\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# pull counts\nx <-  d |>\n  select_if(is.numeric) |># keep only the numeric columns\n  mutate_all(funs(log2(1 + .))) |># log2 transform\n  as.matrix() # matrix\n\nrownames(x) <- d$gene_symbol\n\nx <- t(scale(t(x))) # scale\n\n# pairwise pearson correlation\np <- ???\n\n# heatmap\npheatmap(\n  mat = p,\n  clustering_distance_rows = \"??\",\n  clustering_distance_cols = \"??\",\n  clustering_method = \"??\"\n)\n```\n:::\n\n\n## PCA\n\nPCA is a common dimensionality reduction method that is used to visualize the similarity and differences in your data.\n\n**Let's watch this fantastic 5 minute video explaining PCA**\n\n\n{{< video https://www.youtube.com/embed/HMOI_lkzW08 >}}\n\n\n\nFor more detailed explanations go [here](https://www.nature.com/articles/nbt0308-303) and [here](https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/).\n\n## PCA\n\n\n::: {.cell output-location='column-fragment'}\n\n```{.r .cell-code}\n# pairwise pearson correlation\npc <- \n\nsummary(pc) # summarize the PCs by variance\n```\n:::\n\n\n## PCA - prepare visualization\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# create a dataframe with the importance/explanation of variation for each PC\npca_data_info <- summary(pc)$importance |>as.data.frame()\n\npca_data_info <- round(x = pca_data_info, digits = 3)\n\n# we make a dataframe out of the rotations and will use this to plot\npca_plot_data <- pc$rotation |>\n  as.data.frame() |>\n  rownames_to_column(var = \"ID\") |>\n  separate(\n    ID,\n    into = c(\"time\",\"rep\"),\n    sep = \"_\"\n  )\n\n# recode \"rep\"\npca_plot_data$rep <- recode(\n  pca_plot_data$rep,\n  rep1 = \"A\",\n  rep2 = \"B\",\n  rep3 = \"C\"\n)\n\n# gsub hour\npca_plot_data$time <- gsub(\n  pattern = \"hour\",\n  replacement = \"\",\n  x = pca_plot_data$time\n)\n\nggplot(\n  pca_plot_data, \n  aes(x=PC1, y = PC2, color=time)\n  ) + \n  geom_point() +\n  xlab(paste(\"PC1, %\",100 * pca_data_info[\"Proportion of Variance\",\"PC1\"])) +\n  ylab(paste(\"PC2, %\",100 * pca_data_info[\"Proportion of Variance\",\"PC2\"])) + \n  ggtitle(\"PCA for DUX4 timecourse\") +\n  theme_cowplot()\n```\n:::\n\n\n## Famous PCA example {.smaller}\n\nUsing gene expression as your measurement, do you think the mouse liver is more similar to a mouse heart or a human liver?\n\nThe Mouse ENCODE Consortium reported that comparative gene expression data from human and mouse tend to cluster more by species rather than by tissue.\n\n![](/img/bad_pca.jpg)\n\n[A comparative encyclopedia of DNA elements in the mouse genome](https://www.nature.com/articles/nature13992)\n\n[Comparison of the transcriptional landscapes between human and mouse tissues](https://www.pnas.org/doi/full/10.1073/pnas.1413624111)\n\n## Some found this hard to believe {.smaller}\n\n[Yoav Gilad's lab recapitulated the initial result:](https://f1000research.com/articles/4-121#ref-1)\n\n![](/img/redo_encode.jpg)\n\nThis observation was surprising, as it contradicted much of the comparative gene regulatory data collected previously, as well as the common notion that major developmental pathways are highly conserved across a wide range of species, in particular across mammals.\n\n## Careful with batch effects {.smaller}\n\nBut noticed something funny about which samples were sequenced on the same lanes.\n\n![](/img/batch.jpg)\n\n## Accounting for batch effects {.smaller}\n\n![](/img/fixencode.jpg)\n\nHere we show that the Mouse ENCODE gene expression data were collected using a flawed study design, which confounded sequencing batch (namely, the assignment of samples to sequencing flowcells and lanes) with species. When we account for the batch effect, the corrected comparative gene expression data from human and mouse tend to cluster by tissue, not by species.\n\n## K-means clustering to look for patterns {.smaller}\n\nGoal: to partition `n` observations into `k` clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. --Wiki\n\n**K-means** <iframe width=\"300\" height=\"150\" src=\"https://www.youtube.com/embed/4b5d3muPQmA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## K-Means data preparation {.smaller}\n\n1.  Rows are observations (individuals) and columns are variables\n2.  Any missing value in the data must be removed or estimated.\n3.  The data must be standardized (i.e., scaled) to make variables comparable.\n\n## Scaling or z-score {.smaller}\n\n![](http://www.z-table.com/uploads/2/1/7/9/21795380/1426878678.png){%20}\n\n$x$ = observation\\\n$\\mu$ = population mean\\\n$\\sigma$ = population sd\n\nWe will be using this function on each row. This will allow comparison of relative changes across a row, for all rows.\n\n## K-Means clustering\n\n1.  Computing k-means clustering in R (pheatmap)\n2.  Determine appropriate cluster number\n3.  Add new column with cluster number to initial data\n\n## How do we figure out the optimal \\# clusters?\n\nThere are many methods, but we will stick with the \"elbow\" method.\n\nK-means is minimizing the total within cluster sum of squares (wss).\n\nWe pick the cluster where that drop in total reaches diminishing returns -\\> the elbow.\n\n## Let's cluster once to see {.smaller}\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nset.seed(33)\ntmp <- pheatmap(\n  mat = x,\n  clustering_distance_rows = \"euclidean\",\n  clustering_method = \"ward.D2\",\n  kmeans_k = ??,\n  cluster_cols = FALSE,\n  scale = \"none\"\n)\n\n\ntmp$kmeans$tot.withinss\n```\n:::\n\n\n## Functions in R {.smaller}\n\n![](https://www.learnbyexample.org/wp-content/uploads/r/r-function-syntax.png)\n\n## Create function to calculate wss {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwss <- function(knum) {\n  ph <- pheatmap(\n    mat = x,\n    kmeans_k = ??,\n    scale = \"none\",\n    cluster_cols = FALSE,\n    clustering_distance_rows = \"euclidean\",\n    clustering_method = \"ward.D2\",\n    silent = TRUE\n  )\n  return(ph$kmeans$tot.withinss)\n}\n\nwss(6)\n```\n:::\n\n\n## find the elbow {.smaller}\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\ntibble(wss=map_vec(2:15,wss),\n       k=2:15) |>\n  ggplot(., aes(x=k, y=wss)) +\n  geom_point() +\n  theme_cowplot()\n```\n:::\n\n\n## Final clustering {.smaller}\n\n\n::: {.cell output-location='fragment'}\n\n```{.r .cell-code}\nset.seed(33)\nc <- pheatmap(\n  mat = x,\n  clustering_distance_rows = \"euclidean\",\n  clustering_method = \"ward.D2\",\n  kmeans_k = 7,\n  cluster_cols = F,\n  scale = \"none\"\n)\n\ncg <- tibble(\n  Cluster=c$kmeans$cluster,\n  gene_symbol=names(c$kmeans$cluster)\n)\n\ncd <- left_join(d, cg, by=\"gene_symbol\") \n```\n:::\n\n\n## Which cluster(s) contains DUX4 targets? {.smaller}\n\n[Fisher's Exact Test and the Hypergeometric Distribution](https://www.youtube.com/watch?v=udyAvvaMjfM)\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# list of genes by dux4 targeting\nduxList <- split(cd$gene_symbol, cd$??)\n\n# list of genes by clustering\nclustList <- split(cd$gene_symbol, as.factor(cd$??))\n\n# calculate all overlaps between lists\ngom.duxclust <- newGOM(??List,\n                       ??List,\n                       genome.size = ??)\n\n\ngetMatrix(gom.duxclust, \"pval\") |>\n  t() |>\n  as.data.frame() |>\n  rownames_to_column(var = \"clust\") |>\n  as.tibble() |>\n  arrange(target)\n```\n:::\n\n\n## Let's calculate the empirical p-value of the cluster most enriched for DUX4 targets by sampling {.smaller}\n\nIn order to do this, you will need to:\n\n1.  Identify which cluster is the most enriched for DUX4 targets.\n    -   Determine how many genes are in the cluster. You will need to know this to figure out how many genes to sample from the whole data set.\n    -   Determine how many of the genes in the cluster are DUX4 targets. This is the metric that you are interested in comparing between the null distribution and your observation.\n2.  Generate 1000 random sample of the proper size from all genes and find out how many of them are DUX4 targets.\n3.  Visualize the distribution of DUX4 targets in these 1000 random (your null distribution) and overlay the number of DUX4 targets you observed in the cluster that was most enriched for DUX4 targets.\n",
    "supporting": [
      "ex-15_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}